---
categories: 
  - Math 146
  - Linear Algebra
---

> Known problem: if latex command does not render, please refresh the page. Thanks.

### Basis.
***Definition.***\\
A linearly independent set of vectors in $V$ which spans the space $V$.

***Technique.***\\
To show $S \subseteq V$ is a basis of $V$, we need to prove two things. First, $S$ is a linearly independent set, and second, vectors in $S$ spans $V$.
- First step: Let $S := \\{\alpha_1, \alpha_2, \ldots, \alpha_n\\}.$ We want to show that if $c_1\alpha_1 + \cdots + c_n\alpha_n = 0$, then $c_i = 0$ for all $1 \leq i \leq n$. 
- Second step: Suppose $Y \in V$, then we can express $Y$ in terms of $\alpha_1, \alpha_2, \ldots, \alpha_n$.\\

***Example.***
The columns of an invertible $n\times n$ matrix form a basis for $\mathbb{F}^n$.\\
Let $A\in M_{n,n}(\mathbb{F})$ be invertible. Then it is row-equivalent to the identity matrix $I_n$. Let $C_1, C_2, \ldots, C_n$ denotes the columns of $I_n$. Then $k_1C_1 + k_2C_2 + \ldots + k_nC_n = 0$ implies $k_i = 0$ for all $1 \leq i \leq n$. Since $A$ is row-equivalent to $I$, $m_1A_1 + m_2A_2 + \ldots + m_nA_n = 0$ implies $m_i = 0$. Therefore the columns of $A$ are linearly independent.

To say $\\{A_1, A_2, \ldots, A_n\\}$ form a basis for $\mathbb{F}^n$, we are saying that each $v \in \mathbb{F}^n$ can be written as a unique linear combination of $\\{A_1, A_2, \ldots, A_n\\}$. In other words, the equation $Ax = v$ must have a unique solution. Since $A$ is invertible, there exists a solution $x = A^{-1}v \in \mathbb{F}^n$. Suppose $Ax = v$ and $Ay = v$, then $x = A^{-1}v = y$, which means the solution is unique. 


Hence, the columns of an invertible $n \times n$ matrix form a basis for $\mathbb{F}^n$.

***Example.***\\
Let $A$ be an $m \times n$ matrix and let $S$ be the solution space for the homogeneous system $AX = 0$. Let $R$ be RRE equivalent to $A$. Then $S$ is also the solution space for the system $RX = 0$. If $R$ has $r$ non-zero rows, then the system of equations $RX = 0$ simply expresses $r$ of the unkowns $x_1, \dots, x_n$ in terms of the remaining $(n-r)$ unkowns $x_j$. 

------

### Hoffman 2.3 Example 15
> Let $A$ be an $m \times n$ matrix and let $S$ be the solution space for the homogeneous system $AX = 0$. 

$A$ is an arbitrary $m \times n$ matrix, and $S$ contains all vectors (denoted by $X$) which satisfy $AX = 0$ (thus we say $X$ is in the solution space).

> Let $R$ be a RRE matrix which is equivalent to $A$. Then $S$ is also the solution space for the system $RX = 0$.

Row-equivalent matrices have the same solution space.

> If $R$ has $r$ non-zero rows, then the system of equations $RX = 0$ simply expresses $r$ of the unknowns $x_1, \ldots, x_n$ in terms of the remaining $(n-r)$ unknowns $x_j$. Suppose that the leading non-zero entries of the non-zero rows occur in columns $k_1, \ldots, k_r$. Let $J$ be the set consisting of the $(n-r)$ indices different from $k_1, \ldots, k_r$, ie. $J = \\{1, \ldots, n\\} - \\{k_1, \ldots, k_r}}. The system $RX = 0$ has the form
$$\begin{align*} x_{k1} + \sum_J c_{ij}x_j &= 0 \\ &\vdots \\ x_{kr} + \sum_J c_{rj}x_j &= 0 \end{align*}$$
where $c_{ij}$ are certain scalars. 

Say, the first $r$ rows of $R$ are non-zero, and the rest $(n-r)$ rows are zero. 
